import streamlit as st
import os
import pandas as pd
from dotenv import load_dotenv
import tiktoken
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from summa.summarizer import summarize

# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
api_version = os.getenv("OPENAI_API_VERSION")

#  2. Streamlit ì•± ì‹œì‘
st.set_page_config(page_title="RAG ê¸°ë°˜ ì£¼ì‹ ì±—ë´‡", page_icon="ğŸ“ˆ")
st.title("ğŸ“ˆ ì´ë²ˆ ì£¼ ì£¼ëª©í•  ì£¼ì‹ ì±—ë´‡")

#  3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []


def compress_document(doc: Document, ratio: float = 0.3) -> Document:
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ë§Œ ìš”ì•½í•˜ê³  ì œëª©ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
    :param doc: ìš”ì•½í•  Document ê°ì²´
    :param ratio: ìš”ì•½ ë¹„ìœ¨ (0.2ëŠ” ì•½ 20%ë¡œ ì••ì¶•)
    :return: ìš”ì•½ëœ Document ê°ì²´
    """
    original_content = doc.page_content
    title = doc.metadata.get("source", "ì œëª© ì—†ìŒ")

    # ë³¸ë¬¸ ìš”ì•½
    compressed_content = summarize(original_content, ratio=ratio)

    # ì˜ˆì™¸ ì²˜ë¦¬: ìš”ì•½ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° ì›ë¬¸ ì‚¬ìš©
    if not compressed_content or len(compressed_content.strip()) < 50:
        compressed_content = original_content

    return Document(page_content=compressed_content, metadata={"source": title})


#  4. CSV ë¡œë”© í•¨ìˆ˜
@st.cache_resource
def load_news_csv(csv_path="news.csv"):
    df = pd.read_csv(csv_path)
    documents = []

    encoding = tiktoken.get_encoding("cl100k_base")
    total_tokens = 0

    for idx, row in df.iterrows():
        title = str(row[0]).strip()  # ì œëª©
        content = str(row[1]).strip()  # ë³¸ë¬¸

        # ì œëª©ì€ ê·¸ëŒ€ë¡œ, ë³¸ë¬¸ë§Œ Documentë¡œ
        doc = Document(page_content=content, metadata={"source": title})
        compressed_doc = compress_document(doc, ratio=0.2)

        # í† í° ìˆ˜ ê³„ì‚° (ì••ì¶• í›„)
        token_count = len(encoding.encode(compressed_doc.page_content))
        total_tokens += token_count

        documents.append(compressed_doc)

    print(f"ì „ì²´ ìš”ì•½ í›„ í† í° ìˆ˜ í•©ê³„: {total_tokens}")
    return documents



#  5. ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„± (ìºì‹±)
@st.cache_resource
def setup_retriever():
    documents = load_news_csv("yfinance_articles.csv")
    print(f"Number of chunks: {len(documents)}")
    embeddings = AzureOpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma.from_documents(documents, embeddings)
    return vectorstore.as_retriever()

retriever = setup_retriever()




#  6. í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt = PromptTemplate.from_template(
    """
ë‹¹ì‹ ì€ ì£¼ì‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤. ê° ë¬¸ì„œì—ëŠ” í•´ë‹¹ ê¸°ì‚¬ ë³¸ë¬¸ê³¼ í•¨ê»˜ [ì¶œì²˜: ê¸°ì‚¬ ì œëª©]ì´ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì´ ê¸°ì‚¬ë“¤ì˜ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì •í™•íˆ ì‘ë‹µí•´ì£¼ì„¸ìš”.

---

ğŸŸ© ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì•„ë˜ì˜ ì¶œë ¥ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:

1. **ì§ˆë¬¸ì´ "ì´ë²ˆ ì£¼ í•«í•œ ì£¼ì‹", "ì£¼ëª©í• ë§Œí•œ ì¢…ëª©", "ì§€ê¸ˆ ê´€ì‹¬ê°€ì§ˆ ì£¼ì‹"ê³¼ ê´€ë ¨ëœ ê²½ìš°**  
â†’ ê° ì¢…ëª©ì— ëŒ€í•´ ì•„ë˜ 4ê°€ì§€ ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”:

    - **ì¢…ëª©ëª…**: ëª…í™•í•œ íšŒì‚¬ëª…ì´ë‚˜ ì£¼ì‹ëª… ë“± ì‹ë³„ ê°€ëŠ¥í•œ ì´ë¦„ (íŠ¹ì • ê¸°ì—…ëª…ì´ ì—†ëŠ” ê²½ìš° í•´ë‹¹ ì¢…ëª©ì€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”)
    - **ì„¤ëª…**: ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆëŠ”ì§€, ì–´ë–¤ ë³€í™”ë‚˜ ì´ë²¤íŠ¸ê°€ ìˆì—ˆëŠ”ì§€
    - **ì´ìœ **: ì™œ ì£¼ëª©í•´ì•¼ í•˜ëŠ”ì§€, ë‰´ìŠ¤ ë‚´ìš©ì„ ì¢…í•©í•œ íŒë‹¨
    - **ì¶œì²˜**: ì°¸ê³ í•œ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ë“¤ (ì—¬ëŸ¬ ì¶œì²˜ê°€ ìˆìœ¼ë©´ ëª¨ë‘ ë‚˜ì—´)

2. **ì§ˆë¬¸ì´ íŠ¹ì • ì¢…ëª©ì— ëŒ€í•œ ê²ƒì¸ ê²½ìš° (ì˜ˆ: "ì‚¼ì„±ì „ì ì–´ë•Œ?", "ì¹´ì¹´ì˜¤ ì „ë§ ì•Œë ¤ì¤˜")**  
â†’ í•´ë‹¹ ì¢…ëª©ì— ëŒ€í•´ ì•„ë˜ 4ê°€ì§€ ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”:

    - **ì¢…ëª©ëª…**: ì§ˆë¬¸í•œ íšŒì‚¬ ë˜ëŠ” ì£¼ì‹ëª… (ëª…í™•í•˜ê²Œ í‘œì‹œ)
    - **ì •ë³´**: ì–´ë–¤ ì‚¬ì—…ì„ í•˜ê³  ìˆìœ¼ë©°, í˜„ì¬ ì–´ë–¤ ìƒí™©ì¸ì§€
    - **ìµœì¢… íŒë‹¨**: ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì  ë‹µë³€, ì „ë§ì´ë‚˜ í•´ì„
    - **ì¶œì²˜**: ì°¸ê³ í•œ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ë“¤ (ì •í™•íˆ ì–´ë–¤ ê¸°ì‚¬ì—ì„œ ë‚˜ì˜¨ ë‚´ìš©ì¸ì§€ ëª…ì‹œ)

---

ğŸ”¸ ë™ì¼í•œ ì¢…ëª©ì— ëŒ€í•œ ì¶œì²˜ê°€ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°, **í•œ ì¢…ëª© ì•„ë˜ì— ì¶œì²˜ë¥¼ ëª¨ë‘ ë‚˜ì—´**í•˜ì„¸ìš”.  
ğŸ”¸ ìš”ì²­ ìˆ˜ë³´ë‹¤ ê²€ìƒ‰ ê²°ê³¼ê°€ ì ì„ ê²½ìš°, **ì°¾ì€ ë§Œí¼ë§Œ ì œê³µ**í•´ì£¼ì„¸ìš”.

---

ğŸ“š ë‰´ìŠ¤ ê¸°ì‚¬ ëª¨ìŒ:
{context}

â“ ì§ˆë¬¸:
{question}

"""
)

#  7. ë¬¸ì„œ í¬ë§·í„°
def format_docs(docs):
    return "\n\n".join(
        f"-{i+1}-\n{doc.page_content}\n[ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}]" 
        for i, doc in enumerate(docs)
    )

#  8. LLMê³¼ RAG ì²´ì¸ êµ¬ì„±
llm = AzureChatOpenAI(
    api_key=api_key,
    azure_endpoint=api_base,
    api_version=api_version,
    deployment_name="gpt-4o-mini",  # Azureì—ì„œ ì„¤ì •í•œ ì´ë¦„
    temperature=0.5
)

rag_chain = (
    RunnableParallel({
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }) | prompt | llm | StrOutputParser()
)

#  9. ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

#  10. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    # ì…ë ¥ ì €ì¥ ë° ì¶œë ¥
    st.chat_message("user").write(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            answer = rag_chain.invoke(user_input)
            st.chat_message("assistant").write(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")







