<<<<<<< HEAD
from tkinter import Image
import streamlit as st
import os
import pandas as pd
from dotenv import load_dotenv
import tiktoken
import yfinance as yf
import pandas as pd
import numpy as np
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate

from summa.summarizer import summarize

from langgraph.graph import StateGraph, END, START
from typing import Literal, TypedDict

# -------------------- 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • --------------------
load_dotenv()
=======

import streamlit as st
import os
import pandas as pd
from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.prompts import PromptTemplate

# â¬›ï¸ 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
load_dotenv()

>>>>>>> 0481fc31e4eb6c70463597a9a707896e8dbcd360
api_key = os.getenv("OPENAI_API_KEY")
api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
api_version = os.getenv("OPENAI_API_VERSION")

<<<<<<< HEAD
# -------------------- 2. Streamlit ì„¤ì • --------------------
st.set_page_config(page_title="ì£¼ì‹PLUS", page_icon="ğŸ“ˆ")
st.title("ğŸ“ˆ TOPPIC")
=======
# â¬›ï¸ 2. Streamlit ì•± ì‹œì‘
st.set_page_config(page_title="RAG ê¸°ë°˜ ì£¼ì‹ ì±—ë´‡", page_icon="ğŸ“ˆ")
st.title("ğŸ“ˆ ì´ë²ˆ ì£¼ ì£¼ëª©í•  ì£¼ì‹ ì±—ë´‡")
>>>>>>> 0481fc31e4eb6c70463597a9a707896e8dbcd360

# â¬›ï¸ 3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

<<<<<<< HEAD
# -------------------- 3. ë¬¸ì„œ ì••ì¶• í•¨ìˆ˜ --------------------
def compress_document(doc: Document, ratio: float = 0.3) -> Document:
    original_content = doc.page_content
    title = doc.metadata.get("source", "ì œëª© ì—†ìŒ")
    compressed_content = summarize(original_content, ratio=ratio)
    if not compressed_content or len(compressed_content.strip()) < 50:
        compressed_content = original_content
    return Document(page_content=compressed_content, metadata={"source": title})


# -------------------- ì‹¤ì‹œê°„ ì£¼ì‹ ì •ë³´ --------------------

def calculate_indicators(df, ticker):
    close = df[('Close', ticker)]
    volume = df[('Volume', ticker)] if ('Volume', ticker) in df.columns else pd.Series(dtype=float)

    df[('Indicators', 'SMA_5')] = close.rolling(window=5).mean()
    df[('Indicators', 'SMA_20')] = close.rolling(window=20).mean()

    # RSI ê³„ì‚°
    delta = close.diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=14).mean()
    avg_loss = loss.rolling(window=14).mean()
    rs = avg_gain / (avg_loss + 1e-6)
    df[('Indicators', 'RSI')] = 100 - (100 / (1 + rs))

    # MACD
    ema12 = close.ewm(span=12, adjust=False).mean()
    ema26 = close.ewm(span=26, adjust=False).mean()
    macd = ema12 - ema26
    signal = macd.ewm(span=9, adjust=False).mean()
    df[('Indicators', 'MACD')] = macd
    df[('Indicators', 'Signal')] = signal

    # ê±°ë˜ëŸ‰ í‰ê· ê³¼ ë¹„êµ
    if not volume.empty and volume.notna().any():
        volume_ma5 = volume.rolling(window=5).mean()
        volume_ratio = volume / (volume_ma5 + 1e-6)
        df[('Indicators', 'Volume_MA5')] = volume_ma5
        df[('Indicators', 'Volume_ratio')] = volume_ratio
    else:
        df[('Indicators', 'Volume')] = 0
        df[('Indicators', 'Volume_MA5')] = 1
        df[('Indicators', 'Volume_ratio')] = 1

    # ì „ì¼ ëŒ€ë¹„ ìˆ˜ìµë¥ 
    df[('Indicators', 'Daily_Return')] = close.pct_change()

    return df

def check_surge_possibility(df, ticker):
    output = ""
    latest = df.iloc[-1]

    conds = {
        "ê³¨ë“ í¬ë¡œìŠ¤": latest[('Indicators', 'SMA_5')] > latest[('Indicators', 'SMA_20')],
        "RSI ì–‘í˜¸": 50 < latest[('Indicators', 'RSI')] < 70,
        "MACD ìƒìŠ¹": latest[('Indicators', 'MACD')] > latest[('Indicators', 'Signal')],
        "ê±°ë˜ëŸ‰ ê¸‰ì¦": latest[('Indicators', 'Volume_ratio')] > 1.5,
        "ì „ì¼ ëŒ€ë¹„ +5%": latest[('Indicators', 'Daily_Return')] > 0.05
    }

    score = sum(conds.values())

    output += "\nğŸ“Š ê¸°ìˆ ì§€í‘œ ê¸°ë°˜ ê¸‰ë“± ê°€ëŠ¥ì„± í‰ê°€:\n"
    for k, v in conds.items():
        output += f" - {k}: {'âœ…' if v else 'âŒ'}\n"

    if score >= 4:
        output += "\nğŸš€ ê¸‰ë“± ê°€ëŠ¥ì„± ë†’ìŒ (ê¸°ìˆ ì§€í‘œ ê¸°ì¤€)\n\n"
    elif score >= 2:
        output += "\nâš ï¸ ì•½í•œ ìƒìŠ¹ ì‹ í˜¸ ì¡´ì¬\n\n"
    else:
        output += "\nğŸ” ê¸‰ë“± ê°€ëŠ¥ì„± ë‚®ìŒ (ê¸°ìˆ ì  ì§€í‘œ ê¸°ì¤€)\n\n"

    return output, conds

def analyze_ticker(ticker, period='7d', interval='15m'):
    output = f"ğŸ” [{ticker}] ë°ì´í„° ë¶„ì„ ì¤‘...\n"

    df = yf.download(ticker, period=period, interval=interval)
    if df.empty:
        output += "âŒ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
        return output, None

    output += f"{df.head().to_string()}\n"

    df = calculate_indicators(df, ticker)
    surge_output, conds = check_surge_possibility(df, ticker)
    output += surge_output
    output += f"\nğŸ“ˆ ìµœê·¼ 5ì¼ê°„ ë°ì´í„°:\n{df.tail(5).to_string()}\n"
    return output
# -------------------- 4. ë‰´ìŠ¤ CSV ë¡œë“œ ë° ë¬¸ì„œ ìƒì„± --------------------
=======
# â¬›ï¸ 4. CSV ë¡œë”© í•¨ìˆ˜
>>>>>>> 0481fc31e4eb6c70463597a9a707896e8dbcd360
@st.cache_resource
def load_news_csv(csv_path="news.csv"):
    df = pd.read_csv(csv_path)
    documents = []
<<<<<<< HEAD
    for _, row in df.iterrows():        
        title = str(row[0]).strip()
        content = str(row[1]).strip()
        doc = Document(page_content=content, metadata={"source": title})
        compressed_doc = compress_document(doc, ratio=0.2)
        documents.append(compressed_doc)
    return documents

# -------------------- 5. ë²¡í„°ìŠ¤í† ì–´ ì„¤ì • --------------------
@st.cache_resource
def setup_retriever():
    documents = load_news_csv("yfinance_articles.csv")
=======
    for idx, row in df.iterrows():
        title = str(row[0]).strip()
        content = str(row[1]).strip()
        doc = Document(page_content=content, metadata={"source": title})
        documents.append(doc)
    return documents

# â¬›ï¸ 5. ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„± (ìºì‹±)
@st.cache_resource
def setup_retriever():
    documents = load_news_csv("news.csv")  # ğŸ‘ˆ ê²½ë¡œ í™•ì¸ í•„ìš”
>>>>>>> 0481fc31e4eb6c70463597a9a707896e8dbcd360
    embeddings = AzureOpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma.from_documents(documents, embeddings)
    return vectorstore.as_retriever()

retriever = setup_retriever()

<<<<<<< HEAD
# -------------------- 6. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ --------------------
prompt_recommendation = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì£¼ì‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤.

ì´ë²ˆ ì£¼ í•«í•œ ì£¼ì‹ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”. ì•„ë˜ 4ê°€ì§€ ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”:

- ì¢…ëª©ëª…
- ì„¤ëª…
- ì´ìœ 
- ì¶œì²˜(ì°¸ê³ í–ˆë˜ ì •í™•í•œ ë‰´ìŠ¤ ì œëª©)

ğŸ“š ë‰´ìŠ¤ ê¸°ì‚¬:
{context}

â“ ì§ˆë¬¸:
{question}
""")

prompt_stock_info = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì£¼ì‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ì™€ ê¸°ìˆ ì  ë¶„ì„ ë¦¬í¬íŠ¸ì…ë‹ˆë‹¤.
ê¸°ìˆ ì  ë¶„ì„ ë¦¬í¬íŠ¸ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ë¦¬í¬íŠ¸ì˜ í‰ê°€ë„ ì¶”ê°€í•´ì£¼ì„¸ìš”
íŠ¹ì • ì¢…ëª© ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì •ë³´ë¥¼ í¬í•¨í•´ ë‹µë³€í•´ ì£¼ì„¸ìš”:

- ì¢…ëª©ëª…
- ì •ë³´
- ìµœì‹ ì£¼ê°€
- ë¹„ì „, ì „ë§, ê·¼ê±°
- ê¸°ìˆ ì  ë¶„ì„ ë¦¬í¬íŠ¸
- ì¶œì²˜(ì •í™•í•œ ë‰´ìŠ¤ ì œëª©)

ğŸ“š ë‰´ìŠ¤ ê¸°ì‚¬:
{context}

ğŸ“‰ ê¸°ìˆ ì  ë¶„ì„ ë¦¬í¬íŠ¸:
{data}

â“ ì§ˆë¬¸:
{question}
""")

# -------------------- 7. ë¬¸ì„œ í¬ë§·í„° --------------------
def format_docs(docs):
    return "\n\n".join(
        f"-{i+1}-\n{doc.page_content}\n[ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}]"
        for i, doc in enumerate(docs)
    )

# -------------------- 8. LangGraph ìƒíƒœ ì •ì˜ --------------------
class RAGState(TypedDict):
    question: str
    context: str
    prompt: str
    data: str
    answer: str

# -------------------- 9. LangGraph ë…¸ë“œ ì •ì˜ --------------------
def retrieve_node(state: RAGState) -> dict:
    docs = retriever.invoke(state["question"])
    return {"context": format_docs(docs)}

def classify_node(state: RAGState) -> dict:
    llm = AzureChatOpenAI(
        api_key=api_key,
        azure_endpoint=api_base,
        api_version=api_version,
        deployment_name="gpt-4o-mini",
        temperature=0.0
    )
    classification_prompt = PromptTemplate.from_template("""
ì•„ë˜ ì§ˆë¬¸ì„ ì½ê³  ì–´ë–¤ ìœ í˜•ì˜ ì§ˆë¬¸ì¸ì§€ íŒŒì•…í•˜ì‹œì˜¤:
- ì¢…í•©ì¶”ì²œ: ì´ë²ˆì£¼ í™”ì œ,ì£¼ëª©ë°›ëŠ” ì£¼ì‹ì˜ ì •ë³´ë‚˜ ì£¼ì‹ ì¶”ì²œì´ë‚˜ íˆ¬ì ì „ëµì— ëŒ€í•œ ì§ˆë¬¸ (ì´ë²ˆì£¼, ì£¼ëª©ë°›ëŠ”,í•«í•œ,ë“±ì˜ í‚¤ì›Œë“œë©´ ë°˜ë“œì‹œ ì¢…í•©ì¶”ì²œìœ¼ë¡œ ë¶„ë¥˜)
        ì¢…í•©ì¶”ì²œ ì˜ˆì‹œ:ì´ë²ˆì£¼ ì£¼ëª©ë°›ì€ ì£¼ì‹, ì–´ë–¤ ì£¼ì‹ì´ ê°€ì¥ í•«í•œê°€ìš”?,íˆ¬ìí•  ë§Œí•œ ì£¼ì‹ì€?
- ì¢…ëª©ì •ë³´: íŠ¹ì • ì£¼ì‹ì´ë‚˜ ì¢…ëª©ì— ëŒ€í•œ ì •ë³´ ìš”ì²­,ë¹„ì „ ,ì „ë§,ê·¼ê±° ë“± 
        ì¢…ëª©ì •ë³´ ì˜ˆì‹œ: ì‚¼ì„±ì „ì ì£¼ê°€, ì• í”Œì˜ ë¹„ì „ê³¼ ì „ë§ì€?, í˜„ëŒ€ì°¨ì˜ ìµœê·¼ ì£¼ê°€ëŠ”?
ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ì˜ ë‘ ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œë§Œ í•´ì£¼ì„¸ìš”:
- ì¢…í•©ì¶”ì²œ
- ì¢…ëª©ì •ë³´

ì§ˆë¬¸:
{question}
""")
    prompt_text = classification_prompt.format(question=state["question"])
    response = llm.invoke(prompt_text)
    print(f"ë¶„ë¥˜ ê²°ê³¼: {response.content.strip()}")
    if "ì¢…í•©ì¶”ì²œ" in response.content.strip():
        print("ì¢…í•©ì¶”ì²œìœ¼ë¡œ ë¶„ë¥˜ë¨")
        return {"type": "ì¢…í•©ì¶”ì²œ"}
    else:
        print("ì¢…ëª©ì •ë³´ë¡œ ë¶„ë¥˜ë¨")
        return {"type": "ì¢…ëª©ì •ë³´"}
    
def prompt_recommendation_node(state: RAGState) -> dict:
    prompt_text = prompt_recommendation.format(
        context=state["context"],
        question=state["question"]
    )
    return {"prompt": prompt_text}

# def prompt_stock_info_node(state: RAGState) -> dict:
#     prompt_text = prompt_stock_info.format(
#         context=state["context"],
#         question=state["question"]
#     )
#     return {"prompt": prompt_text}

def prompt_stock_info_node(state: RAGState) -> dict:
    # 1. ì¢…ëª©ëª… ì¶”ì¶œ
    ticker_name = extract_ticker_name(state["question"])

    # 2. ì¢…ëª© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    try:
        report_text = analyze_ticker(ticker_name)
    except Exception as e:
        report_text = f"{ticker_name}ì— ëŒ€í•œ ê¸°ìˆ  ë¶„ì„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\nì˜¤ë¥˜: {e}"
    print(f"ì¢…ëª© ë¶„ì„ ë¦¬í¬íŠ¸: {report_text}")
    state["data"] = report_text

    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt_text = prompt_stock_info.format(
        context=state["context"],
        data=state["data"],
        question=state["question"]
    )
    return {"prompt": prompt_text}


def llm_node(state: RAGState) -> dict:
    llm = AzureChatOpenAI(
        api_key=api_key,
        azure_endpoint=api_base,
        api_version=api_version,
        deployment_name="gpt-4o-mini",
        temperature=0.5
    )
    response = llm.invoke(state["prompt"])
    return {"answer": response.content}



def extract_ticker_name(question: str) -> str:
    llm = AzureChatOpenAI(
        api_key=api_key,
        azure_endpoint=api_base,
        api_version=api_version,
        deployment_name="gpt-4o-mini",
        temperature=0.2
    )
    prompt = PromptTemplate.from_template("""
ì§ˆë¬¸ì—ì„œ ì£¼ì‹ ì¢…ëª©ëª…ì„ í•œê¸€ì´ë“  ì˜ì–´ë“  ì •í™•í•˜ê²Œ í•œ ê°€ì§€ë§Œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.
ì˜ˆì‹œ: " êµ¬ê¸€ ì£¼ê°€ ì–´ë•Œ?" â†’ "GOOGL", "ì• í”Œì˜ ë¹„ì „ì€?" â†’ "AAPL" , "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ ì „ë§?" â†’ "MSFT"
ë°˜ë“œì‹œ ì¢…ëª©ëª…ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ë¶ˆí•„ìš”í•œ ì„¤ëª… ê¸ˆì§€)

ì§ˆë¬¸:
{question}
""")
    response = llm.invoke(prompt.format(question=question))
    return response.content.strip()

# -------------------- 10. LangGraph êµ¬ì„± --------------------
workflow = StateGraph(RAGState)

workflow.add_node("retrieve", retrieve_node)
workflow.add_node("classify", classify_node)
workflow.add_node("prompt_recommendation", prompt_recommendation_node)
workflow.add_node("prompt_stock_info", prompt_stock_info_node)
workflow.add_node("llm", llm_node)

workflow.add_edge(START,"retrieve")
workflow.add_edge("retrieve", "classify")
workflow.add_conditional_edges(
    "classify",
    lambda state: state["type"],
    {
        "ì¢…í•©ì¶”ì²œ": "prompt_recommendation",
        "ì¢…ëª©ì •ë³´": "prompt_stock_info"
    }
)
workflow.add_edge("prompt_recommendation", "llm")
workflow.add_edge("prompt_stock_info", "llm")
workflow.add_edge("llm", END)

rag_graph = workflow.compile()

# -------------------- 11. ê·¸ë˜í”„ ì´ë¯¸ì§€ ì €ì¥ --------------------
graph = rag_graph.get_graph().draw_mermaid_png()
with open("rag_workflow.png", "wb") as f:
    f.write(graph)

# -------------------- 12. Streamlit ì¸í„°í˜ì´ìŠ¤ --------------------
=======
# â¬›ï¸ 6. í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt = PromptTemplate.from_template(
    """
ë‹¤ìŒì€ ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤. ê° ë¬¸ì„œì—ëŠ” í•´ë‹¹ ê¸°ì‚¬ ë³¸ë¬¸ê³¼ í•¨ê»˜ [ì¶œì²˜: ì œëª©]ì´ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì´ ê¸°ì‚¬ë“¤ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì¤˜
ê° ì¢…ëª©ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•´ì¤˜:
1. ì¢…ëª©ëª… (ê°€ëŠ¥í•˜ë©´ ì¢…ëª©ì½”ë“œ)
2. ì„¤ëª… (ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆëŠ”ì§€)
3. ì´ìœ  (ì™œ ì£¼ëª©í•´ì•¼ í•˜ëŠ”ì§€)
4. ì¶œì²˜: ì°¸ê³ í•œ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©

ì¤‘ë³µëœ ì¶œì²˜ëŠ” ì¢…ëª©ë¼ë¦¬ ê³µìœ í•´ë„ ì¢‹ì•„. ì •í™•íˆ ì–´ë–¤ ë¬¸ì„œì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì•˜ëŠ”ì§€ ì—°ê²°í•´ì„œ ë§í•´ì¤˜.

ë‰´ìŠ¤ ê¸°ì‚¬ ëª¨ìŒ:
{context}

ì§ˆë¬¸:
{question}
"""
)

# â¬›ï¸ 7. ë¬¸ì„œ í¬ë§·í„°
def format_docs(docs):
    return "\n\n".join(
        f"-{i+1}-\n{doc.page_content}\n[ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}]" 
        for i, doc in enumerate(docs)
    )

# â¬›ï¸ 8. LLMê³¼ RAG ì²´ì¸ êµ¬ì„±
llm = AzureChatOpenAI(
    api_key=api_key,
    azure_endpoint=api_base,
    api_version=api_version,
    deployment_name="gpt-4o-mini",  # Azureì—ì„œ ì„¤ì •í•œ ì´ë¦„
    temperature=0.5
)

rag_chain = (
    RunnableParallel({
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }) | prompt | llm | StrOutputParser()
)

# â¬›ï¸ 9. ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
>>>>>>> 0481fc31e4eb6c70463597a9a707896e8dbcd360
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# â¬›ï¸ 10. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")
if user_input:
<<<<<<< HEAD
=======
    # ì…ë ¥ ì €ì¥ ë° ì¶œë ¥
>>>>>>> 0481fc31e4eb6c70463597a9a707896e8dbcd360
    st.chat_message("user").write(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
<<<<<<< HEAD
            result = rag_graph.invoke({"question": user_input})
            answer = result["answer"]
=======
            answer = rag_chain.invoke(user_input)
>>>>>>> 0481fc31e4eb6c70463597a9a707896e8dbcd360
            st.chat_message("assistant").write(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
<<<<<<< HEAD
=======




>>>>>>> 0481fc31e4eb6c70463597a9a707896e8dbcd360
