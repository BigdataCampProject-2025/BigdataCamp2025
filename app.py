import streamlit as st
import os
import pandas as pd
from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.prompts import PromptTemplate

# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
api_version = os.getenv("OPENAI_API_VERSION")

#  2. Streamlit ì•± ì‹œì‘
st.set_page_config(page_title="RAG ê¸°ë°˜ ì£¼ì‹ ì±—ë´‡", page_icon="ğŸ“ˆ")
st.title("ğŸ“ˆ ì´ë²ˆ ì£¼ ì£¼ëª©í•  ì£¼ì‹ ì±—ë´‡")

#  3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

#  4. CSV ë¡œë”© í•¨ìˆ˜
@st.cache_resource
def load_news_csv(csv_path="news.csv"):
    df = pd.read_csv(csv_path)
    documents = []
    for idx, row in df.iterrows():
        title = str(row[0]).strip()
        content = str(row[1]).strip()
        doc = Document(page_content=content, metadata={"source": title})
        documents.append(doc)
    return documents

#  5. ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„± (ìºì‹±)
@st.cache_resource
def setup_retriever():
    documents = load_news_csv("news.csv")  # ğŸ‘ˆ ê²½ë¡œ í™•ì¸ í•„ìš”
    embeddings = AzureOpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma.from_documents(documents, embeddings)
    return vectorstore.as_retriever()

retriever = setup_retriever()

#  6. í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt = PromptTemplate.from_template(
    """
ë‹¤ìŒì€ ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤. ê° ë¬¸ì„œì—ëŠ” í•´ë‹¹ ê¸°ì‚¬ ë³¸ë¬¸ê³¼ í•¨ê»˜ [ì¶œì²˜: ì œëª©]ì´ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì´ ê¸°ì‚¬ë“¤ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì¤˜
ê° ì¢…ëª©ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•´ì¤˜:
1. ì¢…ëª©ëª… (ê°€ëŠ¥í•˜ë©´ ì¢…ëª©ì½”ë“œ)
2. ì„¤ëª… (ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆëŠ”ì§€)
3. ì´ìœ  (ì™œ ì£¼ëª©í•´ì•¼ í•˜ëŠ”ì§€)
4. ì¶œì²˜: ì°¸ê³ í•œ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©

ì¤‘ë³µëœ ì¶œì²˜ëŠ” ì¢…ëª©ë¼ë¦¬ ê³µìœ í•´ë„ ì¢‹ì•„. ì •í™•íˆ ì–´ë–¤ ë¬¸ì„œì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì•˜ëŠ”ì§€ ì—°ê²°í•´ì„œ ë§í•´ì¤˜.

ë‰´ìŠ¤ ê¸°ì‚¬ ëª¨ìŒ:
{context}

ì§ˆë¬¸:
{question}
"""
)

#  7. ë¬¸ì„œ í¬ë§·í„°
def format_docs(docs):
    return "\n\n".join(
        f"-{i+1}-\n{doc.page_content}\n[ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}]" 
        for i, doc in enumerate(docs)
    )

#  8. LLMê³¼ RAG ì²´ì¸ êµ¬ì„±
llm = AzureChatOpenAI(
    api_key=api_key,
    azure_endpoint=api_base,
    api_version=api_version,
    deployment_name="gpt-4o-mini",  # Azureì—ì„œ ì„¤ì •í•œ ì´ë¦„
    temperature=0.5
)

rag_chain = (
    RunnableParallel({
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }) | prompt | llm | StrOutputParser()
)

#  9. ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

#  10. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    # ì…ë ¥ ì €ì¥ ë° ì¶œë ¥
    st.chat_message("user").write(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            answer = rag_chain.invoke(user_input)
            st.chat_message("assistant").write(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")







